{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Data Preprocessing"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "# Import packages\n",
    "import json\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('./../../../data/cleaned_data.csv')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# Load lists of numerical and categorical columns from the static file\n",
    "with open('./../../../data/statics.json') as f:\n",
    "    statics = json.load(f)\n",
    "categorical_columns = statics['categorical_columns']\n",
    "numerical_columns = statics['numerical_columns']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Before we begin the preprocessing, it is necessary to split the data into training and testing sets. This is necessary because every transformation has to be trainined on training data while transformation should be done on training and testing set. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# Separate the target variable from the other data\n",
    "y = df['Attrition']\n",
    "X = df.drop('Attrition', axis=1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "categorical_columns.remove('Attrition')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# Seggregate the data into numerical and categorical variable for training data\n",
    "num_df_train = X_train[numerical_columns]\n",
    "cat_df_train = X_train[categorical_columns]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# Seggregate the data into numerical and categorical variable for testing data\n",
    "num_df_test = X_test[numerical_columns]\n",
    "cat_df_test = X_test[categorical_columns]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preprocessing per data types"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Numerical columns"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let us begin the data preprocessing with the numerical columns. Since some of the columns are positively skewed and they does not belong to the same scale, it would be better to make the their scale common. The transformation that will be used in the MinMaxScaler from the scikit-learn. Mathematically, it can be given as:\n",
    "$$\n",
    "X' = \\frac{X - X_{min}}{X_{max} - X_{min}}\n",
    "$$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "# Scale the data\n",
    "transformer = MinMaxScaler()\n",
    "num_df_train = transformer.fit_transform(num_df_train)\n",
    "num_df_test = transformer.transform(num_df_test)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Categorical columns"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "As far as categorical columns are concerned, they need to be represented by numbers so that machines can process the data. For our data, some columns need ordinal encoding while others need one-hot encoding."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "# Separate the columns into ordinal and one hot columns\n",
    "ordinal_columns = ['Education', 'EnvironmentSatisfaction', 'JobInvolvement', 'JobLevel', 'JobSatisfaction', 'OverTime', 'PerformanceRating', 'RelationshipSatisfaction', 'StockOptionLevel', 'WorkLifeBalance']\n",
    "one_hot_columns = ['BusinessTravel', 'Department', 'EducationField', 'Gender', 'JobRole', 'MaritalStatus']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "ordinal_list = []"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In order to encode columns with some order, we need to first declare which will help the algorithm to encode."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "ordinal_list.append(['Below College', 'College', 'Bachelor', 'Master', 'Doctor']) # Education\n",
    "ordinal_list.append(['Low', 'Medium', 'High', 'Very High']) # EnvironmentSatisfaction\n",
    "ordinal_list.append(['Low', 'Medium', 'High', 'Very High']) # JobInvolvement\n",
    "ordinal_list.append(['level_1', 'level_2', 'level_3', 'level_4', 'level_5']) #JobLevel\n",
    "ordinal_list.append(['Low', 'Medium', 'High', 'Very High']) # JobSatisfaction\n",
    "ordinal_list.append(['No', 'Yes']) # OverTime\n",
    "ordinal_list.append(['Excellent', 'Outstanding']) # PerformanceRating\n",
    "ordinal_list.append(['Low', 'Medium', 'High', 'Very High']) # RelationshipSatisfaction\n",
    "ordinal_list.append(['level_0', 'level_1', 'level_2', 'level_3']) # JobInvolvement\n",
    "ordinal_list.append(['Bad', 'Good', 'Better', 'Best']) # WorkLifeBalance\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "# Apply Ordinal Encoder\n",
    "onc = OrdinalEncoder(categories=ordinal_list)\n",
    "ordinal_cat_df_train = onc.fit_transform(cat_df_train[ordinal_columns])\n",
    "ordinal_cat_df_test = onc.transform(cat_df_test[ordinal_columns])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "ohc = OneHotEncoder()\n",
    "onehot_cat_df_train = ohc.fit_transform(cat_df_train[one_hot_columns])\n",
    "onehot_cat_df_test = ohc.transform(cat_df_test[one_hot_columns])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('hra': conda)"
  },
  "interpreter": {
   "hash": "d88894e6c4ebfb1ff377de8b74f40dafb6f3238951903707a6f17b68b3cee000"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}